{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f26db6a",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbab06cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pipelines.text_generation import TextGenerationPipeline\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import LongformerTokenizer, TFLongformerForMaskedLM, pipeline\n",
    "from transformers.file_utils import PushToHubMixin\n",
    "import json\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1be49b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Check that we're using a GPU\n",
    "print(len(tf.config.list_physical_devices(\"GPU\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f3b586",
   "metadata": {},
   "source": [
    "# Loading Data and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e3ee999",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_json('cleanedRapLyrics.json', encoding='utf-8')\n",
    "data_raw['Lyrics'] = data_raw['Lyrics'].astype('unicode').astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66883c08",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFLongformerForMaskedLM.\n",
      "\n",
      "All the layers of TFLongformerForMaskedLM were initialized from the model checkpoint at allenai/longformer-base-4096.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFLongformerForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "model = TFLongformerForMaskedLM.from_pretrained(\"allenai/longformer-base-4096\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7128a0e6",
   "metadata": {},
   "source": [
    "# EDA/Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "968e9301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28066, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79db82c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SongId</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Album</th>\n",
       "      <th>Song</th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Nate Dogg ft. Kurupt</td>\n",
       "      <td>G-Funk Classics Vol. 1</td>\n",
       "      <td>First We Pray</td>\n",
       "      <td>Black people dont have no, no where to go \\nyo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Nate Dogg ft. Daz</td>\n",
       "      <td>G-Funk Classics Vol. 1/Gang Related soundtrack</td>\n",
       "      <td>These Days</td>\n",
       "      <td>(Chorus) x2 \\nThese days, you gotta be strappe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Nate Dogg</td>\n",
       "      <td>G-Funk Classics Vol. 1 &amp; 2</td>\n",
       "      <td>G-Funk</td>\n",
       "      <td>Chorus: \\n\\nG is for the gang of money I make ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Nate Dogg</td>\n",
       "      <td>G-Funk Classics Vol. 1 (Ghetto Preacher)</td>\n",
       "      <td>Crazy, Dangerous</td>\n",
       "      <td>[Nate Dogg] (Nancy Fletcher) \\nNow life for me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Nate Dogg</td>\n",
       "      <td>G-Funk Classics Vol. 1</td>\n",
       "      <td>The Hardest Man in Town</td>\n",
       "      <td>Will the hardest man please stand, the homie s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SongId                Artist  \\\n",
       "0       1  Nate Dogg ft. Kurupt   \n",
       "1       2     Nate Dogg ft. Daz   \n",
       "2       3             Nate Dogg   \n",
       "3       4             Nate Dogg   \n",
       "4       5             Nate Dogg   \n",
       "\n",
       "                                            Album                     Song  \\\n",
       "0                          G-Funk Classics Vol. 1            First We Pray   \n",
       "1  G-Funk Classics Vol. 1/Gang Related soundtrack               These Days   \n",
       "2                      G-Funk Classics Vol. 1 & 2                   G-Funk   \n",
       "3        G-Funk Classics Vol. 1 (Ghetto Preacher)         Crazy, Dangerous   \n",
       "4                          G-Funk Classics Vol. 1  The Hardest Man in Town   \n",
       "\n",
       "                                              Lyrics  \n",
       "0  Black people dont have no, no where to go \\nyo...  \n",
       "1  (Chorus) x2 \\nThese days, you gotta be strappe...  \n",
       "2  Chorus: \\n\\nG is for the gang of money I make ...  \n",
       "3  [Nate Dogg] (Nancy Fletcher) \\nNow life for me...  \n",
       "4  Will the hardest man please stand, the homie s...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ceab275",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw['Lyrics'] = data_raw['Lyrics'].astype('unicode').astype('object').astype('string')\n",
    "data_raw = data_raw.drop(columns = ['SongId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6a0f8c",
   "metadata": {},
   "source": [
    "Due to memory constraints, we can only train on a subset of the dataset. Let's pick some well-known rappers with good lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66716119",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>Album</th>\n",
       "      <th>Song</th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>Eminem</td>\n",
       "      <td>Infinite</td>\n",
       "      <td>Infinite</td>\n",
       "      <td>Oh yeah, this is Eminem baby, back up in that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>Eminem</td>\n",
       "      <td>Infinite</td>\n",
       "      <td>Tonight</td>\n",
       "      <td>[Women singing] \n",
       "Tonight, Tonight, Tonight, To...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Artist     Album      Song  \\\n",
       "845  Eminem  Infinite  Infinite   \n",
       "850  Eminem  Infinite   Tonight   \n",
       "\n",
       "                                                Lyrics  \n",
       "845  Oh yeah, this is Eminem baby, back up in that ...  \n",
       "850  [Women singing] \n",
       "Tonight, Tonight, Tonight, To...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eminem = data_raw[data_raw['Artist'] == 'Eminem']\n",
    "print(eminem.shape)\n",
    "eminem.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "254d1c87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(145, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>Album</th>\n",
       "      <th>Song</th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Nate Dogg ft. 2Pac</td>\n",
       "      <td>G Funk Classics Vol. 1</td>\n",
       "      <td>Me and My Homies</td>\n",
       "      <td>(Chorus) \n",
       "Me and my homies tho \n",
       "you know we ki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>2Pac ft. Nate Dogg, YGD Tha Top Dawg</td>\n",
       "      <td>Greatest Hits</td>\n",
       "      <td>All About U</td>\n",
       "      <td>You probably crooked as the last trick \n",
       "Wanna ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Artist                   Album  \\\n",
       "9                     Nate Dogg ft. 2Pac  G Funk Classics Vol. 1   \n",
       "91  2Pac ft. Nate Dogg, YGD Tha Top Dawg           Greatest Hits   \n",
       "\n",
       "                Song                                             Lyrics  \n",
       "9   Me and My Homies  (Chorus) \n",
       "Me and my homies tho \n",
       "you know we ki...  \n",
       "91       All About U  You probably crooked as the last trick \n",
       "Wanna ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pac2 = data_raw[data_raw['Artist'].str.contains('2Pac')]\n",
    "pac2 = pd.concat([pac2, data_raw[data_raw['Artist'].str.contains('Tupac Shakur')]])\n",
    "print(pac2.shape)\n",
    "pac2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbe1a630",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>Album</th>\n",
       "      <th>Song</th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>A$AP Rocky ft. Joe Fox, Kanye West</td>\n",
       "      <td>At.Long.Last.A$AP</td>\n",
       "      <td>Jukebox Joints</td>\n",
       "      <td>[Chorus: Joe Fox] \n",
       "And I'm a man of my word, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741</th>\n",
       "      <td>Chris Brown ft. Andr_ 3000, Drake, Fabolous, K...</td>\n",
       "      <td>Deuces (Remix) 12\"</td>\n",
       "      <td>Deuces (Remix)</td>\n",
       "      <td>[Verse One: Drake] \n",
       "What you mean I ain't call...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Artist               Album  \\\n",
       "759                  A$AP Rocky ft. Joe Fox, Kanye West   At.Long.Last.A$AP   \n",
       "1741  Chris Brown ft. Andr_ 3000, Drake, Fabolous, K...  Deuces (Remix) 12\"   \n",
       "\n",
       "                Song                                             Lyrics  \n",
       "759   Jukebox Joints  [Chorus: Joe Fox] \n",
       "And I'm a man of my word, t...  \n",
       "1741  Deuces (Remix)  [Verse One: Drake] \n",
       "What you mean I ain't call...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kanye = data_raw[data_raw['Artist'].str.contains('Kanye')]\n",
    "print(kanye.shape)\n",
    "kanye.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca434712",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(94, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>Album</th>\n",
       "      <th>Song</th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10791</th>\n",
       "      <td>N.W.A. (Eazy-E)</td>\n",
       "      <td>N.W.A. &amp; The Posse</td>\n",
       "      <td>Boyz-N-The-Hood</td>\n",
       "      <td>[Eazy-E] \n",
       "Cruising down the street in my 6-4 \n",
       "...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10792</th>\n",
       "      <td>N.W.A.</td>\n",
       "      <td>N.W.A. and The Posse</td>\n",
       "      <td>8 Ball</td>\n",
       "      <td>(Flavor Flav) \n",
       "Kick that shit! \n",
       "Kick that shit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Artist                 Album             Song  \\\n",
       "10791  N.W.A. (Eazy-E)    N.W.A. & The Posse  Boyz-N-The-Hood   \n",
       "10792           N.W.A.  N.W.A. and The Posse           8 Ball   \n",
       "\n",
       "                                                  Lyrics  \n",
       "10791  [Eazy-E] \n",
       "Cruising down the street in my 6-4 \n",
       "...  \n",
       "10792  (Flavor Flav) \n",
       "Kick that shit! \n",
       "Kick that shit...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icecube = data_raw[data_raw['Artist'].str.contains('N.W.A.')]\n",
    "icecube = pd.concat([icecube, data_raw[data_raw['Artist'].str.contains('Ice Cube')]])\n",
    "print(icecube.shape)\n",
    "icecube.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf10a3ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>Album</th>\n",
       "      <th>Song</th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>Snoop Dogg ft. Jay-Z, Nate Dogg, Soopafly</td>\n",
       "      <td>Paid Tha Cost to be Da Bo$$</td>\n",
       "      <td>Lollipop</td>\n",
       "      <td>Just Blaze! \n",
       "\n",
       "[Snoop] \n",
       "Ehehe, oh really? \n",
       "You ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>Eminem ft. 50 Cent, Ca$his, Dr. Dre, Jay-Z, St...</td>\n",
       "      <td>Syllables 12\"</td>\n",
       "      <td>Syllables</td>\n",
       "      <td>[Intro] \n",
       "It is not about lyrics anymore! \n",
       "It's...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Artist  \\\n",
       "192           Snoop Dogg ft. Jay-Z, Nate Dogg, Soopafly   \n",
       "1173  Eminem ft. 50 Cent, Ca$his, Dr. Dre, Jay-Z, St...   \n",
       "\n",
       "                            Album       Song  \\\n",
       "192   Paid Tha Cost to be Da Bo$$   Lollipop   \n",
       "1173                Syllables 12\"  Syllables   \n",
       "\n",
       "                                                 Lyrics  \n",
       "192   Just Blaze! \n",
       "\n",
       "[Snoop] \n",
       "Ehehe, oh really? \n",
       "You ...  \n",
       "1173  [Intro] \n",
       "It is not about lyrics anymore! \n",
       "It's...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jayz = data_raw[data_raw['Artist'].str.contains('Jay-Z')]\n",
    "print(jayz.shape)\n",
    "jayz.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70e9941f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(392, 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_rappers = pd.concat([pac2, kanye, icecube, jayz]).reset_index() \n",
    "# the model started overfitting to Eminem so we got rid of him\n",
    "good_rappers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e140c925",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDS = good_rappers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a66c06",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Small vocabulary/Transformer approach (unfinished)\n",
    "This will only tokenize the ```small_vocab_size``` most common words in the dataset and ignore everything else. We're also throwing away most punctuation and capitalization. (All we really care about is the words.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "83532587",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "small_vocab_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "b796ca09",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "bads = ''.join([chr(char) for char in range(1, 32)])\n",
    "bads += '(' + ')' + '[' + ']' + '{' + '}' + '+' + '\\\"' + r'\"' + 'w/' + '.' + ',' + '!' + '?' + '-' + ':' + '~' + '_' + '=' + '|' + '>' + '<' + ';'\n",
    "for song in IDS.Lyrics:\n",
    "    song = song.translate(str.maketrans('','',bads)).lower()\n",
    "    words += song.split(' ') + [' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "01ee1c9a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word_counter = Counter(words)\n",
    "common_vocab = word_counter.most_common(n=small_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "2a77213d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fout = open('./common_vocab.txt', 'w')\n",
    "for word in common_vocab:\n",
    "    fout.write(word[0])\n",
    "    fout.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c85086",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The following transformer code comes from https://keras.io/examples/generative/text_generation_with_miniature_gpt/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "46750940",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Mask the upper half of the dot product matrix in self attention.\n",
    "    This prevents flow of information from future tokens to current token.\n",
    "    1's in the lower triangle, counting from the lower right corner.\n",
    "    \"\"\"\n",
    "    i = tf.range(n_dest)[:, None]\n",
    "    j = tf.range(n_src)\n",
    "    m = i >= j - n_src + n_dest\n",
    "    mask = tf.cast(m, dtype)\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "    )\n",
    "    return tf.tile(mask, mult)\n",
    "\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "        out1 = self.layernorm1(inputs + attention_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "8249a0fc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "b1033476",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 6000\n",
    "maxlen = 64  # Max sequence size\n",
    "embed_dim = 256  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "feed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "\n",
    "def build_transformer():\n",
    "    inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
    "    x = transformer_block(x)\n",
    "    outputs = layers.Dense(vocab_size)(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    model.compile(\n",
    "        \"adam\", loss=[loss_fn, None],\n",
    "    )  # No loss and optimization based on word embeddings from transformer block\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a43773",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# Create a dataset from text files\n",
    "text_ds = tf.data.TextLineDataset(filenames)\n",
    "text_ds = text_ds.shuffle(buffer_size=256)\n",
    "text_ds = text_ds.batch(batch_size)\n",
    "\n",
    "# Create a vectorization layer and adapt it to the text\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size - 1,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=maxlen + 1,\n",
    ")\n",
    "vectorize_layer.adapt(text_ds)\n",
    "vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n",
    "\n",
    "\n",
    "def prepare_lm_inputs_labels(text):\n",
    "    \"\"\"\n",
    "    Shift word sequences by 1 position so that the target for position (i) is\n",
    "    word at position (i+1). The model will use all words up till position (i)\n",
    "    to predict the next word.\n",
    "    \"\"\"\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    tokenized_sentences = vectorize_layer(text)\n",
    "    x = tokenized_sentences[:, :-1]\n",
    "    y = tokenized_sentences[:, 1:]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "text_ds = text_ds.map(prepare_lm_inputs_labels)\n",
    "text_ds = text_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6eedfe",
   "metadata": {},
   "source": [
    "# Build Longformer and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1019b6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "MAX_LEN = 64\n",
    "LR = 1e-4\n",
    "LYRICS_INPUT_LEN = 64\n",
    "VOCAB_SIZE = 50265 # Do not change!\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 1 # EPOCHS * BATCH_SIZE gives number of songs used in training\n",
    "LRS = [2.5e-4]*EPOCHS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49adb222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \n",
    "    tokens = tf.keras.layers.Input(shape=(MAX_LEN,), name = 'tokens', dtype=tf.int32)\n",
    "    attention = tf.keras.layers.Input(shape=(MAX_LEN,), name = 'attention', dtype=tf.int32)\n",
    "    # Create a model using tokens and attention as layers\n",
    "    x = model(tokens, attention_mask=attention)\n",
    "    # Create a model using tokens and attention layers as input and \n",
    "    # previously built model as output\n",
    "    model2 = tf.keras.Model(inputs=[tokens,attention], outputs=x)\n",
    "    model2.compile(optimizer = tf.keras.optimizers.Adam(lr = LR),\n",
    "                  loss = [tf.keras.losses.CategoricalCrossentropy()],\n",
    "                  metrics = [tf.keras.metrics.CategoricalAccuracy()])\n",
    "    \n",
    "    return model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "836163b2",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def build_better_model():\n",
    "    # Same as build_model but without attention\n",
    "    tokens = tf.keras.layers.Input(shape=(MAX_LEN,), name = 'tokens', dtype=tf.int32)\n",
    "    # Create a model using tokens\n",
    "    x = model(tokens)\n",
    "    model2 = tf.keras.Model(inputs=[tokens], outputs=x)\n",
    "    model2.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "                  loss = [tf.keras.losses.CategoricalCrossentropy()],\n",
    "                  metrics = [tf.keras.metrics.CategoricalAccuracy()])\n",
    "    \n",
    "    return model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cac853d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of rows * max length of lyrics taken from each row\n",
    "train_tokens = np.zeros((len(IDS),MAX_LEN), dtype='int32')\n",
    "train_attention = np.zeros((len(IDS),MAX_LEN), dtype='int32')\n",
    "targets = np.zeros((len(IDS),MAX_LEN, VOCAB_SIZE), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60e26933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of token arrays from lyrics in data\n",
    "def createTrainingArray(data):\n",
    "  for _index, row in data.iterrows():\n",
    "    tokens = tokenizer(row['Lyrics'], return_tensors=\"tf\", \n",
    "                     is_split_into_words = True, add_prefix_space = True, \n",
    "                     max_length = MAX_LEN, padding=\"max_length\")\n",
    "    train_tokens[_index,] = tokens['input_ids'][0][0:MAX_LEN]\n",
    "    train_attention[_index,] = [1]*MAX_LEN\n",
    "    #tokens['attention_mask'][0][0:MAX_LEN]\n",
    "    # Set targets to one-hot encoded array\n",
    "    for i in range(0, MAX_LEN):\n",
    "      temp = np.zeros(VOCAB_SIZE, dtype=np.int8)\n",
    "      if i+1 < len(tokens['input_ids'][0][0:MAX_LEN]):\n",
    "        temp[tokens['input_ids'][0][i+1]] = 1\n",
    "      targets[_index][i] = temp      \n",
    "    # targets[_index, ] = np.zeros((MAX_LEN, VOCAB_SIZE), dtype='int32')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef72b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "createTrainingArray(good_rappers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac054781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrfn(epoch):\n",
    "    return LRS[epoch]\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "974b51a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint(epoch):\n",
    "    if epoch % 10 == 0:\n",
    "        model3.save_weights(f'./checkpoints/{epoch}epochs')\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = './checkpoints',\n",
    "    save_weights_only = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67ff6ef7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size 352 , Valid size 40\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "# Split into train and validation\n",
    "train_idx = np.random.choice(np.arange(len(IDS)),int(0.9*len(IDS)),replace=False)\n",
    "valid_idx = np.setdiff1d(np.arange(len(IDS)),train_idx)\n",
    "np.random.seed(None)\n",
    "print('Train size',len(train_idx),', Valid size',len(valid_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91298eb5",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e42dc33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1dd0a9c7ee0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = build_model()\n",
    "model3.load_weights('./checkpoints/450epochs')\n",
    "\n",
    "model4 = build_better_model()\n",
    "model4.load_weights('./checkpoints/200epochsm4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267c6961",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4dfc6524",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.00025.\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:431: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[24,513,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node functional_1/tf_longformer_for_masked_lm_1/longformer/encoder/layer_._4/attention/self/Pad (defined at C:\\Users\\Michael\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\transformers\\models\\longformer\\modeling_tf_longformer.py:1125) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_969603]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node functional_1/tf_longformer_for_masked_lm_1/longformer/encoder/layer_._4/attention/self/Pad:\n functional_1/tf_longformer_for_masked_lm_1/longformer/encoder/layer_._4/attention/self/Const (defined at C:\\Users\\Michael\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\transformers\\models\\longformer\\modeling_tf_longformer.py:940)\t\n functional_1/tf_longformer_for_masked_lm_1/longformer/encoder/layer_._4/attention/self/einsum/Einsum (defined at C:\\Users\\Michael\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\transformers\\models\\longformer\\modeling_tf_longformer.py:937)\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-eef78539300d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model3.fit(x = [train_tokens[train_idx,], train_attention[train_idx,]],\n\u001b[0m\u001b[0;32m      2\u001b[0m           \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m           validation_data = ([train_tokens[valid_idx,], train_attention[valid_idx,]],\n\u001b[0;32m      4\u001b[0m                              targets[valid_idx,]),\n\u001b[0;32m      5\u001b[0m           \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlr_callback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[24,513,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node functional_1/tf_longformer_for_masked_lm_1/longformer/encoder/layer_._4/attention/self/Pad (defined at C:\\Users\\Michael\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\transformers\\models\\longformer\\modeling_tf_longformer.py:1125) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_969603]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node functional_1/tf_longformer_for_masked_lm_1/longformer/encoder/layer_._4/attention/self/Pad:\n functional_1/tf_longformer_for_masked_lm_1/longformer/encoder/layer_._4/attention/self/Const (defined at C:\\Users\\Michael\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\transformers\\models\\longformer\\modeling_tf_longformer.py:940)\t\n functional_1/tf_longformer_for_masked_lm_1/longformer/encoder/layer_._4/attention/self/einsum/Einsum (defined at C:\\Users\\Michael\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\transformers\\models\\longformer\\modeling_tf_longformer.py:937)\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "model3.fit(x = [train_tokens[train_idx,], train_attention[train_idx,]],\n",
    "          y = targets[train_idx,],\n",
    "          validation_data = ([train_tokens[valid_idx,], train_attention[valid_idx,]],\n",
    "                             targets[valid_idx,]),\n",
    "          callbacks = [lr_callback, checkpoint_callback],\n",
    "          epochs = EPOCHS,\n",
    "          batch_size = BATCH_SIZE,\n",
    "          verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0bccee",
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:431: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model4.fit(x = [train_tokens[train_idx,]],\n",
    "          y = targets[train_idx,],\n",
    "          validation_data = ([train_tokens[valid_idx,]],\n",
    "                             targets[valid_idx,]),\n",
    "          callbacks = [checkpoint_callback],\n",
    "          epochs = EPOCHS,\n",
    "          batch_size = BATCH_SIZE,\n",
    "          verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd87778",
   "metadata": {},
   "source": [
    "# Generate lyrics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "61f5b3b9",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# DO NOT READ!!! Common curse words\n",
    "expletives = [\"fuck\", \"shit\", \"ass\", \"bitch\", \"whore\", \"slut\", \"nig\"]\n",
    "# These are tokens that we know are bad\n",
    "badTokens = [1437, 50118, 22886, 35625, 524, 1185, 1009, 14783, 4771, 2409, 27741, 216, 7586]\n",
    "bannedTokens = badTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "8ab41c36",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def getOutputWord(logits, _tokenizer):\n",
    "  # Get the words formed by the highest logit tokenizer outputs\n",
    "    words = \"\"\n",
    "    for i in range(0, len(logits)):\n",
    "        words += str(_tokenizer.decode(logits[i], skip_special_tokens=True))\n",
    "    \n",
    "    wordList = words.split(' ')\n",
    "    for i in range(0, len(wordList)):\n",
    "        for expletive in expletives:\n",
    "            if expletive in wordList[i].lower():\n",
    "                wordList[i] = r\"[EXPLETIVE]\"\n",
    "                break\n",
    "    words = ''\n",
    "    for word in wordList:\n",
    "        words += word + \" \"\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "7148437d",
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generateLogitsModel3(prompt):\n",
    "    p = []\n",
    "    pure = tokenizer.encode(prompt)[1:-1]\n",
    "    for i in range(len(prompt), 64):\n",
    "        temp = []\n",
    "        for j in range(0, 64):\n",
    "            if(j < len(pure)):\n",
    "                temp.append(pure[j])\n",
    "            else:\n",
    "                temp.append(50624)\n",
    "        p = model3.predict([np.array([temp]), np.array([[1]*64])], batch_size=16, verbose=2).logits\n",
    "        tempe = 0\n",
    "        tsum = 0\n",
    "        sca = 0.03\n",
    "        k = 0\n",
    "        max_check = -9999\n",
    "        while k < 50265:\n",
    "            if p[0][i][k] > max_check and k not in bannedTokens:\n",
    "                max_check = p[0][i][k]\n",
    "            k += 1\n",
    "        k = 0\n",
    "        while k < 50265:\n",
    "            # print(p[0][i][k])\n",
    "            if k not in bannedTokens:\n",
    "                tsum += np.exp(sca*(p[0][i][k]-max_check))\n",
    "            k += 1\n",
    "        cutoff = np.random.rand()*tsum\n",
    "        result = -1\n",
    "        k = 0\n",
    "        print(i)\n",
    "        # print(max_check)\n",
    "        # print(tsum)\n",
    "        while k < 50265:\n",
    "            # print(p[0][i][k])\n",
    "            if k not in bannedTokens:\n",
    "                tempe += np.exp(sca*(p[0][i][k]-max_check))\n",
    "                if tempe >= cutoff:\n",
    "                    result = k\n",
    "                    break\n",
    "            k += 1\n",
    "        # print(tempe)\n",
    "        pure.append(result)\n",
    "    return pure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "ca716973",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def generateLogitsModel4(prompt):\n",
    "    p = []\n",
    "    pure = tokenizer.encode(prompt)[1:-1]\n",
    "    for i in range(len(prompt), 64):\n",
    "        temp = []\n",
    "        for j in range(0, 64):\n",
    "            if(j < len(pure)):\n",
    "                temp.append(pure[j])\n",
    "            else:\n",
    "                temp.append(50624)\n",
    "        p = model4.predict([np.array([temp])], batch_size=16, verbose=2).logits\n",
    "        tempe = 0\n",
    "        tsum = 0\n",
    "        sca = 0.03\n",
    "        k = 0\n",
    "        max_check = -9999\n",
    "        while k < 50265:\n",
    "            if p[0][i][k] > max_check and k not in bannedTokens:\n",
    "                max_check = p[0][i][k]\n",
    "            k += 1\n",
    "        k = 0\n",
    "        while k < 50265:\n",
    "            # print(p[0][i][k])\n",
    "            if k not in bannedTokens:\n",
    "                tsum += np.exp(sca*(p[0][i][k]-max_check))\n",
    "            k += 1\n",
    "        cutoff = np.random.rand()*tsum\n",
    "        result = -1\n",
    "        k = 0\n",
    "        print(i)\n",
    "        # print(max_check)\n",
    "        # print(tsum)\n",
    "        while k < 50265:\n",
    "            # print(p[0][i][k])\n",
    "            if k not in bannedTokens:\n",
    "                tempe += np.exp(sca*(p[0][i][k]-max_check))\n",
    "                if tempe >= cutoff:\n",
    "                    result = k\n",
    "                    break\n",
    "            k += 1\n",
    "        # print(tempe)\n",
    "        pure.append(result)\n",
    "    return pure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "85346b18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 0s\n",
      "8\n",
      "1/1 - 0s\n",
      "9\n",
      "1/1 - 0s\n",
      "10\n",
      "1/1 - 0s\n",
      "11\n",
      "1/1 - 0s\n",
      "12\n",
      "1/1 - 0s\n",
      "13\n",
      "1/1 - 0s\n",
      "14\n",
      "1/1 - 0s\n",
      "15\n",
      "1/1 - 0s\n",
      "16\n",
      "1/1 - 0s\n",
      "17\n",
      "1/1 - 0s\n",
      "18\n",
      "1/1 - 0s\n",
      "19\n",
      "1/1 - 0s\n",
      "20\n",
      "1/1 - 0s\n",
      "21\n",
      "1/1 - 0s\n",
      "22\n",
      "1/1 - 0s\n",
      "23\n",
      "1/1 - 0s\n",
      "24\n",
      "1/1 - 0s\n",
      "25\n",
      "1/1 - 0s\n",
      "26\n",
      "1/1 - 0s\n",
      "27\n",
      "1/1 - 0s\n",
      "28\n",
      "1/1 - 0s\n",
      "29\n",
      "1/1 - 0s\n",
      "30\n",
      "1/1 - 0s\n",
      "31\n",
      "1/1 - 0s\n",
      "32\n",
      "1/1 - 0s\n",
      "33\n",
      "1/1 - 0s\n",
      "34\n",
      "1/1 - 0s\n",
      "35\n",
      "1/1 - 0s\n",
      "36\n",
      "1/1 - 0s\n",
      "37\n",
      "1/1 - 0s\n",
      "38\n",
      "1/1 - 0s\n",
      "39\n",
      "1/1 - 0s\n",
      "40\n",
      "1/1 - 0s\n",
      "41\n",
      "1/1 - 0s\n",
      "42\n",
      "1/1 - 0s\n",
      "43\n",
      "1/1 - 0s\n",
      "44\n",
      "1/1 - 0s\n",
      "45\n",
      "1/1 - 0s\n",
      "46\n",
      "1/1 - 0s\n",
      "47\n",
      "1/1 - 0s\n",
      "48\n",
      "1/1 - 0s\n",
      "49\n",
      "1/1 - 0s\n",
      "50\n",
      "1/1 - 0s\n",
      "51\n",
      "1/1 - 0s\n",
      "52\n",
      "1/1 - 0s\n",
      "53\n",
      "1/1 - 0s\n",
      "54\n",
      "1/1 - 0s\n",
      "55\n",
      "1/1 - 0s\n",
      "56\n",
      "1/1 - 0s\n",
      "57\n",
      "1/1 - 0s\n",
      "58\n",
      "1/1 - 0s\n",
      "59\n",
      "1/1 - 0s\n",
      "60\n",
      "1/1 - 0s\n",
      "61\n",
      "1/1 - 0s\n",
      "62\n",
      "1/1 - 0s\n",
      "63\n"
     ]
    }
   ],
   "source": [
    "m3logits = generateLogitsModel3(\"Ok ok ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "22facf45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 0s\n",
      "2\n",
      "1/1 - 0s\n",
      "3\n",
      "1/1 - 0s\n",
      "4\n",
      "1/1 - 0s\n",
      "5\n",
      "1/1 - 0s\n",
      "6\n",
      "1/1 - 0s\n",
      "7\n",
      "1/1 - 0s\n",
      "8\n",
      "1/1 - 0s\n",
      "9\n",
      "1/1 - 0s\n",
      "10\n",
      "1/1 - 0s\n",
      "11\n",
      "1/1 - 0s\n",
      "12\n",
      "1/1 - 0s\n",
      "13\n",
      "1/1 - 0s\n",
      "14\n",
      "1/1 - 0s\n",
      "15\n",
      "1/1 - 0s\n",
      "16\n",
      "1/1 - 0s\n",
      "17\n",
      "1/1 - 0s\n",
      "18\n",
      "1/1 - 0s\n",
      "19\n",
      "1/1 - 0s\n",
      "20\n",
      "1/1 - 0s\n",
      "21\n",
      "1/1 - 0s\n",
      "22\n",
      "1/1 - 0s\n",
      "23\n",
      "1/1 - 0s\n",
      "24\n",
      "1/1 - 0s\n",
      "25\n",
      "1/1 - 0s\n",
      "26\n",
      "1/1 - 0s\n",
      "27\n",
      "1/1 - 0s\n",
      "28\n",
      "1/1 - 0s\n",
      "29\n",
      "1/1 - 0s\n",
      "30\n",
      "1/1 - 0s\n",
      "31\n",
      "1/1 - 0s\n",
      "32\n",
      "1/1 - 0s\n",
      "33\n",
      "1/1 - 0s\n",
      "34\n",
      "1/1 - 0s\n",
      "35\n",
      "1/1 - 0s\n",
      "36\n",
      "1/1 - 0s\n",
      "37\n",
      "1/1 - 0s\n",
      "38\n",
      "1/1 - 0s\n",
      "39\n",
      "1/1 - 0s\n",
      "40\n",
      "1/1 - 0s\n",
      "41\n",
      "1/1 - 0s\n",
      "42\n",
      "1/1 - 0s\n",
      "43\n",
      "1/1 - 0s\n",
      "44\n",
      "1/1 - 0s\n",
      "45\n",
      "1/1 - 0s\n",
      "46\n",
      "1/1 - 0s\n",
      "47\n",
      "1/1 - 0s\n",
      "48\n",
      "1/1 - 0s\n",
      "49\n",
      "1/1 - 0s\n",
      "50\n",
      "1/1 - 0s\n",
      "51\n",
      "1/1 - 0s\n",
      "52\n",
      "1/1 - 0s\n",
      "53\n",
      "1/1 - 0s\n",
      "54\n",
      "1/1 - 0s\n",
      "55\n",
      "1/1 - 0s\n",
      "56\n",
      "1/1 - 0s\n",
      "57\n",
      "1/1 - 0s\n",
      "58\n",
      "1/1 - 0s\n",
      "59\n",
      "1/1 - 0s\n",
      "60\n",
      "1/1 - 0s\n",
      "61\n",
      "1/1 - 0s\n",
      "62\n",
      "1/1 - 0s\n",
      "63\n"
     ]
    }
   ],
   "source": [
    "m4logits = generateLogitsModel4(\"Ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c098eec9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok ok ok [EXPLETIVE] like can canThe wanna like mother Eminem man get do\t got do get say got like like Eminem got can go like man like like get like got like Eminem like\t like like\t doVer get got like startKick get like never like{ like [EXPLETIVE] got got \n"
     ]
    }
   ],
   "source": [
    "print(getOutputWord(m3logits, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "f58090ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok [EXPLETIVE] can get like mother can like\t can like got yeah like Eminem got can never Eminem unimaginable\t right doemate\tVer mother do love can canVer Lose go likeH can never say feel like [EXPLETIVE] can Bristol got get\t [EXPLETIVE] like can get canThat can got\t\tVer get like don Eminem \n"
     ]
    }
   ],
   "source": [
    "print(getOutputWord(m4logits, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
